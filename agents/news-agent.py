from typing import Annotated, Sequence, TypedDict

########### OPEN AI ####################
from langchain_core.messages import BaseMessage # Foundational class for all message types in Langgraph
from langchain_core.messages import ToolMessage # Passes data class back to LLM after it calls a tool such as the content and tool call id
from langchain_core.messages import SystemMessage  # Message providing instructions to the LLM
from langchain_core.messages import HumanMessage # Passing HumanMessage
from langchain_core.messages import AIMessage # Message generated by the LLM
from langchain_openai import ChatOpenAI

##### Tools #############
from langchain_core.tools import tool
from langchain_tavily import TavilySearch


######## Langgraph Components #########
from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode


######### ENV #########
from dotenv import load_dotenv

##### Load API KEYS & SECRETS #######
load_dotenv()


class AgentState(TypedDict):
    ##### HumanMessage and AIMessage --> Datatypes for langchain
    messages : Annotated[Sequence[BaseMessage], add_messages]
    initialize_chat : int



######## Initialize Tools and Models ##########

# list of all tools

tools = [TavilySearch(max_results=5,topic="news")]

##### Model (Binding Tools) #########
llm = ChatOpenAI(model="gpt-4o").bind_tools(tools)



# AGENT NODE FUNCTION ######
def ReAct(state: AgentState) -> AgentState:

    ####### define system prompt ########
    system_prompt = SystemMessage(content=
   "You are my AI Assistant named RYO, please introduce yourself and answer my query to the best of your ability")

    
    response = llm.invoke([system_prompt] + state["messages"])
    
    return {"messages": [response]}



def react_function(state: AgentState) -> AgentState:
    messages = state["messages"]
    last_message = messages[-1]
    if not last_message.tool_calls:
        return "end"
    else:
        return "continue"
    


graph = StateGraph(AgentState)

###### (A) Add nodes ######  

### 1. Chat Nodes ####

graph.add_node("ReAct Node", ReAct)

### 2. Tool Node #####
tool_node = ToolNode(tools=tools)
graph.add_node("tools", tool_node)


#### (B) Add edges ######

graph.add_edge(START, "ReAct Node")


graph.add_conditional_edges(
    "ReAct Node",
    react_function,
    {
        "continue": "tools",
        "end" : END
    }
)

graph.add_edge("tools", "ReAct Node")


agent =  graph.compile()
agent



# Helper Function 
def stream_agent(stream, show_stream=True):
    output = []
    for s in stream:
        message = s["messages"][-1]
        if show_stream==True:
            if isinstance(message, tuple):
                print(message)
            else:
                message.pretty_print()
            
        output.append(message)

    return output



conversation_history = []
initialize_chat  = 1
user_input = input("Ask Ryo Anything!: ")
while user_input!= "exit":

    conversation_history.append(HumanMessage(content=user_input))
 
    if user_input == "":
        conversation_history.append(HumanMessage(content="Nothing was typed by the user can you ask again what they wanted based on the previous context."))

    inputs = {"messages": [("user", "\n".join([i.content for i in conversation_history]))],
                "initialize_chat" : initialize_chat} 


    result = stream_agent(agent.stream(inputs, stream_mode="values"), show_stream=False)

    print("AI:", result[-1].content)
    
    conversation_history  = [i for i in result if i.type!='tool']

    initialize_chat  = 0
    
    user_input = input("Enter: ")




with open("outputs/logging.txt", "w") as file:
    
    for message in conversation_history:
        if isinstance(message, HumanMessage):
            file.write(f"You: {message.content}\n")
        elif isinstance(message, AIMessage):
            file.write(f"AI: {message.content}\n\n")
    file.write("End of Conversation")

print("Conversation saved to logging.txt")