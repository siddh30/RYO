{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7484b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6218fa",
   "metadata": {},
   "source": [
    "# Main Supervisor (The CEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a406a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# supervisor = create_supervisor(\n",
    "#     model=init_chat_model(\"openai:gpt-4.1\"),\n",
    "#     supervisor_name= 'The CEO',\n",
    "#     agents=[research_agent, math_agent],\n",
    "#     prompt=(\n",
    "#         \"You are a supervisor managing two agents:\\n\"\n",
    "#         \"- a research agent. Assign research-related tasks to this agent\\n\"\n",
    "#         \"- a math agent. Assign math-related tasks to this agent\\n\"\n",
    "#         \"Assign work to one agent at a time, do not call agents in parallel.\\n\"\n",
    "#         \"Do not do any work yourself.\"\n",
    "#     ),\n",
    "#     add_handoff_back_messages=True,\n",
    "#     output_mode=\"full_history\",\n",
    "# ).compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b6fb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddharthmandgi/anaconda3/envs/AgenticAI-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "class PersistentRAG:\n",
    "    def __init__(self, storage_file=\"rag_memory.json\"):\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Small, fast embedding model\n",
    "        self.memory = []\n",
    "        self.storage_file = storage_file\n",
    "        self.load_memory()\n",
    "\n",
    "    def save(self, text, tags=None):\n",
    "        \"\"\"\n",
    "        Save text, tags, timestamp, and embedding to memory.\n",
    "        \"\"\"\n",
    "        embedding = self.model.encode(text).tolist()\n",
    "        entry = {\n",
    "            \"text\": text,\n",
    "            \"tags\": tags or [],\n",
    "            \"timestamp\": time.time(),\n",
    "            \"embedding\": embedding\n",
    "        }\n",
    "        self.memory.append(entry)\n",
    "        self.persist_memory()\n",
    "\n",
    "    def retrieve(self, query, top_k=3, tag_filter=None):\n",
    "        \"\"\"\n",
    "        Retrieve top_k most relevant texts based on embedding similarity.\n",
    "        Optionally filter by tag.\n",
    "        \"\"\"\n",
    "        query_embedding = self.model.encode(query)\n",
    "        \n",
    "        # Apply tag filter if provided\n",
    "        candidates = [\n",
    "            entry for entry in self.memory \n",
    "            if tag_filter is None or any(tag in entry[\"tags\"] for tag in tag_filter)\n",
    "        ]\n",
    "        \n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "        # Compute similarity scores\n",
    "        embeddings = [entry[\"embedding\"] for entry in candidates]\n",
    "        scores = util.cos_sim(query_embedding, embeddings)[0]\n",
    "\n",
    "        # Rank by score\n",
    "        ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
    "        return ranked[:top_k]\n",
    "\n",
    "    def persist_memory(self):\n",
    "        \"\"\"\n",
    "        Save memory to disk as JSON.\n",
    "        \"\"\"\n",
    "        with open(self.storage_file, \"w\") as f:\n",
    "            json.dump(self.memory, f)\n",
    "\n",
    "    def load_memory(self):\n",
    "        \"\"\"\n",
    "        Load memory from disk if it exists.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.storage_file, \"r\") as f:\n",
    "                self.memory = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548dda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved: The capital of France is Paris. (Score: 0.8790)\n",
      "Retrieved: The capital of France is Paris. (Score: 0.8790)\n"
     ]
    }
   ],
   "source": [
    "# Save some entries\n",
    "\n",
    "rag = PersistentRAG()\n",
    "rag.save(\"The capital of France is Paris.\", tags=[\"geography\"])\n",
    "rag.save(\"Python is a popular programming language.\", tags=[\"programming\"])\n",
    "rag.save(\"OpenAI develops artificial intelligence.\", tags=[\"AI\"])\n",
    "\n",
    "# Retrieve with query\n",
    "results = rag.retrieve(\"What is the capital of France?\", top_k=2)\n",
    "\n",
    "for entry, score in results:\n",
    "    print(f\"Retrieved: {entry['text']} (Score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716cba5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticAI-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
